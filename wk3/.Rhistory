summary(lm.chas)
head(Boston)
lm.nox=lm(crim~nox)
summary(lm.nox)
head(Boston)
lm.rm=lm(crim~rm)
summary(lm.rm)
head(Boston)
lm.age=lm(crim~age)
lm.dis=lm(crim~dis)
lm.rad=lm(crim~rad)
unique(rad)
dim(Boston)
?count
lenth(unique(tax))
length(unique(tax))
lm.tax=lm(crim~tax)
head(Boston)
lm.pratio=lm(crim~pratio)
lm.ptratio=lm(crim~ptratio)
lm.black=lm(crim~black)
lm.lstat=lm(crim~lstat)
lm.medv=lm(crim~medv)
summary(lm.chas)
lmBoston=lm(crim~.,Boston)
summary(lmBoston)
summary(lm.zn)
coefficients(lm.zn)
coefficients(lm.zn)%>%class
head(Boston)
length(coefficients(lmBoston))
exp=c(coefficients(lm.zn)[2], coefficients(lm.indus)[2], coefficients(lm.chas)[2], coefficients(lm.nox)[2], coefficients(lm.rm)[2], coefficients(lm.age)[2], coefficients(lm.dis)[2], coefficients(lm.rad)[2], coefficients(lm.tax)[2], coefficients(lm.ptratio)[2], coefficients(lm.black)[2], coefficients(lm.lstat)[2], coefficients(lm.medv)[2])
resp=coefficients(lm(Boston))[2:14]
plot(exp,resp)
plot.new()
plot.new()
plot(exp,resp)
df<-data.frame(exp,resp)
df
lm.indus
lmBoston
lm.nox
lmZN=lm(crim ~ poly(zn,3))
lmZn
lmZn=lmZn
lmZn=lmZn
lmZn=lmZN
rm(lmZn)
summary(lmZN)
lmIndus=lm(crim~poly(indus,3))
summary(lmIndus)
lmBlack=lm(crim~poly(black,3))
summary(lmBlack)
lmChas=lm(crim~poly(chas,3))
head(Boston)
summary(lm.chas)
lm.chas
summary(lm.chas)
80.88+.106*99
-837.38+2917.60*.297
-837.38+2917.60*.297+1514.29*.37
teamRank = c(1,2,3,3,4,4,4,4,5,5)
wins2012=c(94,88,95,88,93,94,98,97,93,94)
cor(teamRank,wins2012)
wins2013=c(97,97,92,93,92,96,94,96,92,90)
cor(teamRank,wins2013)
getwd()
setwd("DataSci_R/AnEdge/")
dir()
nutr=read.csv("USDA.csv")
nutr%>%head
library(magrittr)
nutr%>%head
fix(nutr)
attach(nutr)
lm(Cholesterol~TotalFat+SaturatedFat,data=nutr)
summary(nutr)
lm(Cholesterol~TotalFat+SaturatedFat,data=nutr)%>%summary
wine=read.csv('~/Downloads/wine.csv')
getwd()
getwd()
dir()
setwd("wk1")
dir()
wine=read.csv()
wine=read.csv('wine.csv')
head(wine.csv)
head(wine)
m = lm(Price~HarvestRain+WinterRain, data=wine)
summary(m)
cor(wine$HarvestRain,wine$WinterRain)
dir()
load('wk1Notes.RData')
head(CPS)
tapply(is.na(CPS$MetroAreaCode), CPS$State, mean)
with(CPS, tapply(is.na(MetroAreaCode),State,mean))
a=with(CPS, tapply(is.na(MetroAreaCode),State,mean))
b=tapply(is.na(CPS$MetroAreaCode), CPS$State, mean)
identical(a,b)
abs(0.30-with(CPS, tapply(is.na(MetroAreaCode),State,mean)))%>%sort()
abs(0.30-with(CPS, tapply(is.na(MetroAreaCode),State,mean)))%>%sort()%>%head(1)%>%names
fix(CPS)
head(CPS)
metCodes=read.csv('MetroAreaCodes.csv')
nrow(metCodes)
countryCodes=read.csv('CountryCodes.csv')
nrow(countryCodes)
head(CPS)
head(metCodes)
nrow(CPS)
nrow(metCodes)
?merge
CPS2=merge(CPS,metCodes,by.x="MetroAreaCode",by.y="Code",all.x=T)
nrow(CPS2)
sum(is.na(CPS2$MetroAreaCode))
head(CPS2)
tapply(CPS2$MetroAreaCode,CPS2$MetroArea,function(x)length(x))
tapply(CPS2$MetroAreaCode,CPS2$MetroArea,function(x)length(x))%>%head(1)%>%names()
tapply(CPS2$MetroAreaCode,CPS2$MetroArea,function(x)length(x))%>%sort(decreasing = T)%>%head
tapply(CPS2$MetroAreaCode,CPS2$MetroArea,function(x)length(x))%>%sort(decreasing = T)%>%head(1)%>%names()
tapply(CPS2$MetroAreaCode,CPS2$MetroArea,function(x)length(x))%>%sort(decreasing = T)%>%head(10)
tapply(CPS2$MetroAreaCode,CPS2$MetroArea,function(x)length(x))%>%sort(decreasing = T)%>%head(10)
tapply(CPS2$MetroAreaCode,CPS2$MetroArea,function(x)length(x))%>%sort(decreasing = T)%>%head(15)
head(CPS2)
with(CPS2, tapply(Hispanic,MetroArea,mean))%>%sort(decreasing = T)%>%head()
with(CPS2, tapply(Hispanic,MetroArea,mean))%>%sort(decreasing = T)%>%head(15)
with(CPS2, tapply(Hispanic,MetroArea,mean))%>%sort(decreasing = T)%>%head(20)
with(CPS2, tapply(Hispanic,MetroArea,mean))%>%sort(decreasing = T)%>%head(30)
CPS2%>%head()
with(CPS2, tapply(Race==Asian, MetroArea,mean))%>%sort(decreasing = T)%>%head()
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%head()
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%head(10)
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%head(15)
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%head(18)
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%head(20)
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%head(25)
wash=subset(CPS2,grepl('Washington',CPS2$MetroArea))
head(wash)
wash$MetroArea%>%unique
wash$MetroArea=factor(wash$MetroArea)
wash$MetroArea%>%unique
mean(wash$Race=='Asian')
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%head(5)
with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)%>%class()
?subset
asianPct=with(CPS2, tapply(Race=="Asian", MetroArea,mean))%>%sort(decreasing = T)
asianPct[asianPct>0.20]
sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean))
sort(tapply(CPS2$Education == "No high school diploma", CPS2$MetroArea, mean))
sort(tapply(CPS2$Education == "No high school diploma", CPS2$MetroArea, mean, na.rm=T))
sort(tapply(CPS2$Education == "No high school diploma", CPS2$MetroArea, mean, na.rm=T))%>%head()
sort(tapply(CPS2$Education == "No high school diploma", CPS2$MetroArea, mean, na.rm=T))%>%head(1)
head(countryMap)
head(countryCodes
)
head(CPS)
CPS3=merge(CPS,countryCodes,by.x="MetroAreaCode",by.y="Code",all.x=T)
head(CPS3)
nrow(CPS)
nrow(CPS3)
sum(is.na(CPS$Country))
sum(is.na(CPS3$Country))
head(CPS)
CPS3=merge(CPS,countryCodes,by.x="CountryOfBirthCode",by.y="Code",all.x=T)
head(CPS3)
sum(is.na(CPS3$Country))
CPS3=merge(CPS2,countryCodes,by.x="CountryOfBirthCode",by.y="Code",all.x=T)
sum(is.na(CPS3$Country))
nyc=subset(CPS3, grepl('Long Island',MetroArea))
unique(nyc$MetroArea)
nyc$MetroArea=factor(nyc$MetroArea)
nyc$MetroArea
table(nyc$Country)
table(nyc$Country)%>%sort(decreasing = T)%>%head()
table(nyc$Country)%>%sort(decreasing = T)%>%head()%>%names()
mean(nyc$Country=='United States')
mean(nyc$Country=='United States',na.rm=T)
mean(nyc$Country!='United States',na.rm=T)
with(CPS3,tapply(CountryOfBirth=="India",MetroArea,mean))%>%sort(decreasing = T)%>%head()
with(CPS3,tapply(Country=="India",MetroArea,mean))%>%sort(decreasing = T)%>%head()
with(CPS3,tapply(Country=="India",MetroArea,mean))%>%sort(decreasing = T)%>%head(10)
with(CPS3,tapply(Country=="India",MetroArea,mean, na.rm=T))%>%sort(decreasing = T)%>%head(10)
with(CPS3,tapply(Country=="India",MetroArea,mean, na.rm=T))%>%sort(decreasing = T)%>%head(20)
with(CPS3,tapply(Country=="Brazil",MetroArea,mean, na.rm=T))%>%sort(decreasing = T)%>%head(20)
with(CPS3,tapply(Country=="Somalia",MetroArea,mean, na.rm=T))%>%sort(decreasing = T)%>%head(20)
sort(tapply(CPS3$Country == "India", CPS$MetroArea, sum, na.rm=TRUE))%>%head()
setwd("../wk2/")
climate=read.csv('climate_change.csv')
head(climate)
train=subset(climate,Year<=2006)
test=subset(climate,Year>2006)
rm(list=setdiff(ls(), c("climate","train","test"))
)
mod1=lm(Temp~.-Year-Month)
mod1=lm(Temp~.-Year-Month,train)
summary(mod1)
CH4AndCFC11=lm(Temp~CH4+CFC.11,train)
summary(CH4AndCFC11)
cor(train$CH4,train$CFC.11)
cor(train)
cor(train)%>%class
cor(train)[1]
cor(train)[1,1]
mod2=lm(Temp~N2O+MEI+TSI+Aerosols)
mod2=lm(Temp~N2O+MEI+TSI+Aerosols,train)
summary(mod2)
step(mod2)
modx=lm(Temp~.,train)
modx
step(modx)
step(modx)%>%summary
modx%>%summary
mod2
mod1
step(mod1)
step(mod1)%>%summary
head(train)
train$Year%>%sort%>%tail(1)
lm(Temp~.-Year-Month,data=train)
lm(Temp~.-Year-Month,data=train)%>%summary()
climateLM=lm(Temp~.-Year-Month,data=train)
climateLM=lm(Temp~.-Year-Month,data=train)%>%step
stepped=step(climateLM)
summary(stepped)
climPred=predict(stepped,newData=test)
str(climPred)
dim(test)
dim(climPred)
length(climPred)
NBA=read.csv('NBA_train.csv')
head(NBA)
PointsReg4 = lm(PTS~X2PA + X3PA + FTA + AST + ORB + STL, data=NBA)
summary(PointsReg4)
NBA_test= read.csv('NBA_test.csv')
dim(NBA)
dim(NBA_test)
PointsPredictions = predict(PointsReg4, newdata = NBA_test)
str(PointsPredictions)
SSE = sum((NBA_test$PTS - PointsPredictions)^2)
SST = sum(NBA_test$PTS - mean(NBA$PTS)^2)
R2_OOS_NBA=1-SSE/SST
R2_OOS_NBA
SSE
SST
SST = sum((NBA_test$PTS - mean(NBA$PTS))^2)
R2_OOS_NBA=1-SSE/SST
R2_OOS_NBA
summary(stepped)
head(test)
climPred = predict(stepped, newData=test)
str(climPred)
dim(climate)
subset(climate,Year<=2006)%>%str
initialModel = lm(Temp ~ . - Month - Year, data = train)
stepped(initialModel)%>%summary
step(initialModel)%>%summary
?predict
length(climPred)
str(climPred)
class(climPred)
dir()
pisaTrain = read.csv('pisa2009train.csv')
pisaTest = read.csv('pisa2009test.csv')
head(pisaTrain)
dim(pisaTrain)
with(pisaTrain,tapply(readingScore,male,mean))
?any
v=c(4,NA,1,NA,3)
any(is.na(v))
all(is.na(V))
all(is.na(v))
with(pisaTrain,sapply(function(x)any(is.na(x))))
sapply(pisaTrain,function(x)any(is.na(x)))
sapply(pisaTrain,function(x)any(is.na(x)))%>%which
pisaTrain = na.omit(pisaTrain)
nrow(pisaTrain)
pisaTest = na.omit(pisaTest)
nrow(pisaTest)
pisaTrain$raceeth%>%str
pisaTrain$raceeth%>%head
pisaTrain$raceeth%>%levels()
pisaTrain$raceeth%>%relevel('White')%>%levels()
pisaTrain$raceeth%>%levels()
pisaTest$raceeth%>%levels()
pisaTrain$raceeth = relevel(pisaTrain$raceeth, "White")
pisaTest$raceeth = relevel(pisaTest$raceeth, "White")
pisaReadingModel = lm(ReadingScore ~ .,data = pisaTrain)
names(pisaTrain)
pisaReadingModel = lm(readingScore ~ .,data = pisaTrain)
summary(pisaTrain)
summary(pisaReadingModel)
lmScore = lm(readingScore~., data=pisaTrain)
rm(pisaReadingModel)
summary(lmScore)
str(lmScore$residuals)
dim(pisaTrain)
SSE = sum(lmScore$residuals^2)
RMSE = sqrt(SSE/nrow(pisaTrain))
RMSE
summary(lmScore)
predTest = predict(lmScore, newdata = pisaTest)
length(predTest)
dim(pisaTest)
dim(pisaTrain)
predTest%>%range()
predTest%>%min()
predTest%>%max()
x=predTest%>%range()
x[2]-x[1]
SSE = sum((pisaTest$readingScore - predTest)^2)
RMSE = sqrt(SSE/nrow(pisaTest))
SSE
RMSE
pisaTrain$readingScore%>%mean()
sum((pisaTrain$readingScore - mean(pisaTrain$readingScore))^2
)
sum((pisaTrain$readingScore - mean(pisaTest$readingScore))^2
)
sum((pisaTest$readingScore - mean(pisaTrain$readingScore))^2)
sum((pisaTest$readingScore - mean(pisaTrain$readingScore))^2)
SST = sum((pisaTest$readingScore - mean(pisaTrain$readingScore))^2)
1-SsE/SST
1-SSE/SST
dir()
fluTrain=read.csv('FluTrain.csv')
str(fluTrain)
head(fluTrain)
which.max(fluTrain$ILI)
fluTrain[303,]
search()
library(dplyr)
fluTrain%>%order(desc(ILI))%>%head(1)
?order
fluTrain%>%arrange(desc(ILI))%>%head(1)
fluTrain%>%arrange(desc(Query))%>%head(1)
fluTrain%>%arrange(desc(Queries))%>%head(1)
hist(fluTrain$ILI)
plot.new()
hist(fluTrain$ILI)
?log
with(fluTrain,plot(Queries, log(ILI)))
flu1=lm(log(ILI)~Queries,data=fluTrain)
summary(flu1)
fluTest=read.csv('FluTest.csv')
fluTest%>%head
ILI_pred=predict(flu1, newdata=fluTest)%>%exp
str(ILI_pred)
dim(fluTest)
grep('2012-03-11',fluTest$Week,values=T)
?grep
grep('2012-03-11',fluTest$Week,value=T)
grep('2012-03-11',fluTest$Week)
fluTest[11,]
ILI_pred[11]
ILI_pred
(fluTest[11,2]-ILI_pred[11])/(fluTest[11,2])
SSE=sum((fluTest$ILI-ILI_pred)^2)
RMSE = sqrt(SSE/nrow(ILI))
RMSE = sqrt(SSE/nrow(fluTest))
RMSE
max(ILI_Pred$ILI)
max(fluTest$ILI)
summary(fluTest$ILI)
find.package('zoo')
install.packages('zoo')
library(zoo)
ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)
ILILag2 = lag(zoo(fluTrain$ILI), -2, na.pad=TRUE)
fluTrain$ILILag2 = coredata(ILILag2)
?lag
ILILag2 = lag(zoo(fluTrain$ILI), -2, na.pad=TRUE)
class(-2)
ILILag2 = lag(zoo(fluTrain$ILI), 2, na.pad=TRUE)
summary(ILILag2)
str(ILILag2)
ILILag2 = stats::lag(zoo(fluTrain$ILI), 2, na.pad=TRUE)
ILILag2 = stats::lag(zoo(fluTrain$ILI), -2, na.pad=TRUE)
ILILag2
str(ILILag2)
plot(log(ILILag2),log(ILI))
str(fluTrain)
FluTrain$ILILag2 = coredata(ILILag2)
fluTrain$ILILag2 = coredata(ILILag2)
str(fluTrain)
plot(fluTrain$ILI,fluTrain$ILILag2)
flu1
flu1%>%summary
flu2 = lm(ILI~Queries+ILILag2,data=fluTrain)
summary(flu2)
flu2 = lm(log(ILI)~Queries+log(ILILag2),data=fluTrain)
flu2%>%summary
flu2TestPred=predict(flu2,fluTest)
nrow(fluTrain)
nrow(fluTest)
flu2TestPred=predict(flu2,newdata=fluTest)
ILILag2_test = stats::lag(zoo(fluTest$ILI),-2,na.pad=T)
fluTest$ILILag2 = coredata(ILILag2_test)
sum(is.na(fluTest$ILILag2))
fluTest$ILILag2%>%head
head(fluTrain)
sapply(fluTrain,class)
tail(fluTrain)
head(fluTest)
tail(fluTest)
fluTest$ILILag2[1]=fluTrain$ILILag2[length(fluTrain)-1]
fluTest$ILILag2[2]=fluTrain$ILILag2[length(fluTrain)]
tail(fluTrain)
head(fluTest)
fluTest$ILILag2[1]=fluTrain$ILI[length(fluTrain)-1]
fluTest$ILILag2[2]=fluTrain$ILI[length(fluTrain)]
tail(fluTrain)
head(fluTest)
head(fluTrain)
fluTest$ILILag2[2]=fluTrain$ILI[nrow(fluTrain)]
fluTest$ILILag2[1]=fluTrain$ILI[nrow(fluTrain)-1]
fluTrain%>%tail
fluTrain%>%head
fluTest%>%head
flu2testPreds = predict(flu2, newdata=fluTest)
flu2testPreds = predict(flu2, newdata=fluTest)%>%exp()
head(flu2testPreds)
SSE2 = sum((flu2testPreds-fluTest$ILI)^2)
RMSE2 = sqrt(SSE2/nrow(fluTest))
RMSE2
climate = read.csv('climate_change.csv')
train = subset(climate,Year<=2006) # has 284 observations
test = subset(climate, Year>2006) # has 24 observations
initialModel = lm(Temp ~ . - Month - Year, data = train)
stepped = step(initialModel) #R^2 for this model is the correct value of 0.7508
climPred = predict(stepped, newdata = test)
length(climPred) # returns 284
exp(-1)
exp(1)
x<-exp(-1)
x/(1+x)
search()
getwd()
setwd("../wk3")
find.package('datasets')
?sample.split
find.package('caTools')
library(caTools)
?sample.split
head(mtcars)
library(faraway)
head(worldcup)
table(worldcup$Position)
wcPosns = tapply(worldcup$Team,worldcup$Position,function(x)length(x)/nrow(worldcup))
wcPosns
sum(wcPosns)
wcPosnsPcts=wcPosns
rm(wcPosns)
wcPosnsPcts
length(worldcup$Position)
wcPosn_split = sample.split(worldcup$Position,SplitRatio=0.70)
str(wcPosn_split)
head(wcPosn_split,20)
sum(wcPosn_split)/length(wcPosn_split)
nrow(worldcup)
wcTrain = subset(worldcup, wcPosn_split)
wcTest = subset(worldcup, !wcPosn_split)
wcPosnsPcts
tapply(wcTrain$Team, wcTrain$Position, function(x)length(x)/nrow(wcTrain))
tapply(wcTest$Team, wcTest$Position, function(x)length(x)/nrow(wcTest))
# Read in dataset
quality = read.csv("quality.csv")
# Look at structure
str(quality)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75) # sample.split is from library caTools
split # split is a logical vector with 0.75 TRUE values and it's the same length as quality$PoorCare
# the splitting was performed such that the distribution of labels among the indices in quality$PoorCare
# whose value  in 'split' is TRUE is equal to that among the indices in quality$PoorCare whose
# value in 'split' is FALSE; both are equal to the original distribution of labels in the entire
# quality$PoorCare set.
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# Make predictions on training set
predictTrain = predict(QualityLog, type="response")
# Analyze predictions
summary(predictTrain)
print('printing predicted vs actual results')
tapply(predictTrain, qualityTrain$PoorCare, mean)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
names(quality)
QualityLog = glm(PoorCare ~ StartedOnCombination + ProviderCount, data=qualityTrain, family = binomial)
summary(QualityLog)
table(quality$StartedOnCombination)
predictTrain
table(qualityTrain$PoorCare, predictTrain > 0.5)
table(qualityTrain$PoorCare, predictTrain > 0.7)
table(qualityTrain$PoorCare, predictTrain > 0.2)
search()
find.package("ROCR")
install.packages("ROCR")
library(ROCR)
?prediction
str(predictTrain)
predictTrain = predict(QualityLog, type="response")
str(predictTrain)
?performance
?prediction
?performance
ROCRpred = prediction(predictTrain, quality$PoorCare)
