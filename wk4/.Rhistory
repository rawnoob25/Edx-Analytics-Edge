isB_testTable = table(test$isB, predict(isB_classificationTree, newdata=test, type='class'))
isB_testTable
(1134+299)/(1134+299+41+84)
CARTb = rpart(isB ~ . - letter, data=train, method="class")
predictions = predict(CARTb, newdata=test, type="class")
table(test$isB, predictions)
letters = read.csv('letters_ABPR.csv')
letters$isB = as.factor(letters$letter == "B")
set.seed(1000)
split = sample.split(letters$isB, SplitRatio=0.50)
train = subset(letters, split)
test = subset(letters, !split)
summary(letters)
isB_classificationTree = rpart(isB ~ .-letter, data = train, method="class")
isB_testTable = table(test$isB, predict(isB_classificationTree, newdata=test, type='class'))
isB_testTable
tot = sum(isB_testTable)
(1119+340/tot)
(1119+340)/tot
(1119+340)/nrow(test)
(1118+340)/nrow(test)
library(magrittr)
library(dplyr)
library(caTools) #for splitting into train/test sets
library(ROCR) #for computing AUCs
library(rpart) #contains rpart function used to build classification and regression trees
library(rpart.plot) #contains prp function used to plot classification and regression trees
library(randomForest) #for building random forest models
letters = read.csv('letters_ABPR.csv')
letters$isB = as.factor(letters$letter == "B")
set.seed(1000)
split = sample.split(letters$isB, SplitRatio=0.50)
train = subset(letters, split)
test = subset(letters, !split)
summary(letters)
isB_classificationTree = rpart(isB ~ .-letter, data = train, method="class")
isB_testTable = table(test$isB, predict(isB_classificationTree, newdata=test, type='class'))
isB_testTable
set.seed(1000)
isB_classificationForest = randomForest(isB ~ .-letter, data=train)
letters = read.csv('letters_ABPR.csv')
letters$isB = as.factor(letters$letter == "B")
set.seed(1000)
split = sample.split(letters$isB, SplitRatio=0.50)
train = subset(letters, split)
test = subset(letters, !split)
summary(letters)
isB_classificationTree = rpart(isB ~ .-letter, data = train, method="class")
isB_testTable = table(test$isB, predict(isB_classificationTree, newdata=test, type='class'))
isB_testTable
set.seed(1000)
isB_classificationForest = randomForest(isB ~ .-letter, data=train)
table(test$isB, predict(isB_classificationForest,newdata=test, type='class'))
nrow(letters)
(1163+374)/nrow(test)
sapply(letters, class)
class(letters$letter)
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(lettes, !split)
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
CARTmodel = rpart(letters ~ .-isB, data=train, method='class')
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
CARTmodel = rpart(letter ~ .-isB, data=train, method='class')
table(test$letter, predict(CARTmodel, newdata=test, type='class'))
t=table(test$letter, predict(CARTmodel, newdata=test, type='class'))
nrow(t)
for (year in c(2010,2011,2012,2013,2014,2015)){
print(paste("The year is", year))
}
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
CARTmodel = rpart(letter ~ .-isB, data=train, method='class')
t=table(test$letter, predict(CARTmodel, newdata=test, type='class'))
tot=0
for (i in seq(nrow(t))){
tot = tot+t[i,i]
}
tot/sum(t)
args(rpart)
args(randomForest)
?randomForest
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
CARTmodel = rpart(letter ~ .-isB, data=train, method='class')
t=table(test$letter, predict(CARTmodel, newdata=test, type='class'))
tot=0
for (i in seq(nrow(t))){
tot = tot+t[i,i]
}
tot/sum(t)
set.seed(1000)
randomForestModel = randomForest(letter ~ .-isB, data=train)
table(test$letter, predict(randomForestModel, newdata=test, type='class'))
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
CARTmodel = rpart(letter ~ .-isB, data=train, method='class')
cartTbl=table(test$letter, predict(CARTmodel, newdata=test, type='class'))
computeDiag=function(t){
tot=0
for (i in seq(nrow(t))){
tot = tot+t[i,i]
}
}
computeDiag(cartTbl)/sum(cartTbl)
set.seed(1000)
randomForestModel = randomForest(letter ~ .-isB, data=train)
randomForestTbl=table(test$letter, predict(randomForestModel, newdata=test, type='class'))
computDiag(randomForestTbl)/sum(randomForestTbl)
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
CARTmodel = rpart(letter ~ .-isB, data=train, method='class')
cartTbl=table(test$letter, predict(CARTmodel, newdata=test, type='class'))
computeDiag=function(t){
tot=0
for (i in seq(nrow(t))){
tot = tot+t[i,i]
}
}
computeDiag(cartTbl)/sum(cartTbl)
set.seed(1000)
randomForestModel = randomForest(letter ~ .-isB, data=train)
randomForestTbl=table(test$letter, predict(randomForestModel, newdata=test, type='class'))
computeDiag(randomForestTbl)/sum(randomForestTbl)
set.seed(2000)
split=sample.split(letters$letter, SplitRatio =0.50)
train = subset(letters, split)
test = subset(letters, !split)
table(train$letter)
table(test$letter)
CARTmodel = rpart(letter ~ .-isB, data=train, method='class')
cartTbl=table(test$letter, predict(CARTmodel, newdata=test, type='class'))
computeDiag=function(t){
tot=0
for (i in seq(nrow(t))){
tot = tot+t[i,i]
}
return(tot)
}
computeDiag(cartTbl)/sum(cartTbl)
set.seed(1000)
randomForestModel = randomForest(letter ~ .-isB, data=train)
randomForestTbl=table(test$letter, predict(randomForestModel, newdata=test, type='class'))
computeDiag(randomForestTbl)/sum(randomForestTbl)
census=read.csv("census.csv")
head(census)
census=read.csv("census.csv")
logRegrMod = glm(over50k ~ ., data=census, family='binomial')
summary(logRegrMod)
census=read.csv("census.csv")
split = sample.split(census$over50k, SplitRatio = 0.60)
train = subset(census, split)
test = subset(census, !split)
logRegrMod = glm(over50k ~ ., data=train, family='binomial')
summary(logRegrMod)
census=read.csv("census.csv")
set.seed(2000)
split = sample.split(census$over50k, SplitRatio = 0.60)
train = subset(census, split)
test = subset(census, !split)
logRegrMod = glm(over50k ~ ., data=train, family='binomial')
summary(logRegrMod)
census=read.csv("census.csv")
set.seed(2000)
split = sample.split(census$over50k, SplitRatio = 0.60)
train = subset(census, split)
test = subset(census, !split)
logRegrMod = glm(over50k ~ ., data=train, family='binomial')
summary(logRegrMod)
table(test$over50k, predict(logRegrMod, newdata=test, type='response')>=0.50)
table(test$over50k, predict(logRegrMod, newdata=test, type='response')>=0.50)%>%assign(t)
t
table(test$over50k, predict(logRegrMod, newdata=test, type='response')>=0.50)
(9051+1888)/(9051+1888+1190+662)
table(train$over50k)
table(test$over50k)
9713/(9713+3078)
logRegrModPredictionObj = prediction(predict(logRegrMod, newdata=test, type='response'), test$over50k)
performance(logRegrModPredictionObj, "auc")@y.values%>%as.numeric
censusClassTree = rpart(over50k ~ ., data=train, type="class")
censusClassTree = rpart(over50k ~ ., data=train, method="class")
prp(censusClassTree)
censusClassTree = rpart(over50k ~ ., data=train, method="class")
prp(censusClassTree)
predict(censusClassTree, newdata=test,type='response')%>%str
?predict
args(predict)
formals(predict)
censusClassTree = rpart(over50k ~ ., data=train, method="class")
prp(censusClassTree)
predict(censusClassTree, newdata=test,type='class')%>%str
#table(test$over50k, predict(censusClassTree, newdata=test,type='class'))
#predict(censusClassTree, newdata=test,type='class')%>%str
table(test$over50k, predict(censusClassTree, newdata=test,type='class'))
predict(censusClassTree, newdata=test)%>%str
#table(test$over50k, predict(censusClassTree, newdata=test,type='class'))
#predict(censusClassTree, newdata=test)%>%str #Note: if you leave out
#'type=response', you'll get a vector of probabilities
table(test$over50k, predict(censusClassTree, newdata=test,type='class'))
s=(9243+1596)
tot=sum(table(test$over50k, predict(censusClassTree, newdata=test,type='class')))
s/tot
probabilities = predict(censusclassTree, newdata=test)
probabilities = predict(censusClassTree, newdata=test)
str(probabilities)
probabilities = predict(censusClassTree, newdata=test)
class(probabilities)
probabilities = predict(censusClassTree, newdata=test)
str(probabilities[1])
probabilities = predict(censusClassTree, newdata=test)
dim(probabilities)
probabilities = predict(censusClassTree, newdata=test)
str(probabilities[,1])
probabilities = predict(censusClassTree, newdata=test)
str(probabilities[,2])
probabilities = predict(censusClassTree, newdata=test)
str(probabilities[,1])
probabilities = predict(censusClassTree, newdata=test)
head(probabilities[,1])
probabilities = predict(censusClassTree, newdata=test)
head(probabilities[,2])
probabilities = predict(censusClassTree, newdata=test)
probabilities
probabilities = predict(censusClassTree, newdata=test)
probabilities[1:5,]
probabilities = predict(censusClassTree, newdata=test)
probabilities[1:5,]
over50k = probabilities[,2]
censusCARTpredictionObj = prediction(over50k, test$over50k)
as.numeric(performance(censusCARTpredictionObj,"auc")@y.values)
probabilities = predict(censusClassTree, newdata=test)
probabilities[1:5,]
over50k = probabilities[,2]
censusCARTpredictionObj = prediction(over50k, test$over50k)
censusClassTree_ROC = performance(censusCARTpredictionObj, "tpr", "fpr")
plot(censusClassTree_ROC)
as.numeric(performance(censusCARTpredictionObj,"auc")@y.values)
head(train)
nrow(train)
?sample
sample(50,3)
x <- 1:10
sample(x[x >  8])
sample(x[x >  9])
names(census)
str(over50k)
rm(over50k)
trainSmall = train[sample(nrow(train),2000),]
censusRandomForest = randomForest(over50k ~ ., data=trainSmall)
table(test$over50k, predict(censusRandomForest, newdata=test, type='class'))
correct = 8822+1993
t=table(test$over50k, predict(censusRandomForest, newdata=test, type='class'))
correct/sum(t)
set.seed(1)
trainSmall = train[sample(nrow(train),2000),]
set.seed(1)
censusRandomForest = randomForest(over50k ~ ., data=trainSmall)
table(test$over50k, predict(censusRandomForest, newdata=test, type='class'))
correct = 8843+2049
t=table(test$over50k, predict(censusRandomForest, newdata=test, type='class'))
correct/sum(t)
set.seed(1)
trainSmall = train[sample(nrow(train), 2000), ]
set.seed(1)
censusrf = randomForest(over50k ~ . , data = trainSmall)
predictTest = predict(censusrf, newdata=test)
table(test$over50k, predictTest)
vu = varUsed(censusrf, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(MODEL$forest$xlevels[vusorted$ix]))
vu = varUsed(censusrf, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(censusrf$forest$xlevels[vusorted$ix]))
?varUsed
varImpPlot(censusrf)
?expand.grid
head(census)
set.seed(2)
tr.control=trainControl(method="cv", number=10)
library(magrittr)
library(dplyr)
library(caTools) #for splitting into train/test sets
library(ROCR) #for computing AUCs
library(rpart) #contains rpart function used to build classification and regression trees
library(rpart.plot) #contains prp function used to plot classification and regression trees
library(randomForest) #for building random forest models
library(caret) #for cross-validation
library(e1071) #for cross-validation
set.seed(2)
tr.control=trainControl(method="cv", number=10)
cp.grid =expand.grid(.cp=seq(0.002,1,0.002))
train(over50k~.,data=train,method="rpart",trControl=tr.control,tuneGrid=cp.grid)
set.seed(2)
tr.control=trainControl(method="cv", number=10)
cp.grid =expand.grid(.cp=seq(0.002,0.1,0.002))
train(over50k~.,data=train,method="rpart",trControl=tr.control,tuneGrid=cp.grid)
CARTmodel_best_cp = rpart(over50k~., data=train, method="class", cp=0.002)
table(test$over50k, predict(CARTmodel_best_cp, newdata=test, type="response"))
CARTmodel_best_cp = rpart(over50k~., data=train, method="class", cp=0.002)
table(test$over50k, predict(CARTmodel_best_cp, newdata=test, type="class"))
correct = 9178+535
t=table(test$over50k, predict(CARTmodel_best_cp, newdata=test, type="class"))
correct/sum(t)
correct = 9178+1838
correct/sum(t)
prp(CARTmodel_best_cp)
?randomForest
getwd()
states= read.csv('statedataSimple.csv')
states= read.csv('statedataSimple.csv')
head(states)
dir()
read.csv('statedataSimple.csv')
rm(list=ls())
state=read.csv('statedataSimple.csv')
summary(state)
linMod1 = lm(Life.Exp~.,data=state)
summary(linMod1)
linMod1Preds = predict(linMod1, type=)
linMod1Preds = predict(linMod1)
str(linMod1Preds)
sum((state$Life.Exp-linMod1Pres)^2)
sum((state$Life.Exp-linMod1Preds)^2)
lm(Life.Exp~Murder+HS.Grad, data=state)%>%summary
linMod2=lm(Life.Exp~Murder+HS.Grad+Population+Frost, data=state)
linMod2%>%summary
sum((state$Life.Exp-linMod2Preds)^2)
linMod2Preds = predict(linMod2)
sum((state$Life.Exp-linMod2Preds)^2)
CART1 = rpart(Life.Exp~.,data=state)
prp(CART1)
CART1preds = predict(CART1, type='class')
traceback()
CART1preds = predict(CART1, type='class')
summary(CART1)
clear()
clc()
CARTmodel = rpart(Life.Exp ~ ., data=statedata)
CARTmodel = rpart(Life.Exp ~ ., data=state)
CART1preds = predict(CART1, type='class')
CART1preds = predict(CART1)
str(CART1preds)
sum((CART1preds - state$Life.Exp)^2)
CARTmodel = rpart(Life.Exp ~ ., data=state, minbucket=5)
CART1preds = predict(CART1)
prp(CART1preds)
prp(CARTmodel)
prp(CARTmodel, digits=4)
sum((CART1preds - state$Life.Exp)^2)
CART1preds = predict(CARTmodel)
CARTmodelPreds = predict(CARTmodel)
sum((CARTmodelPreds - state$Life.Exp)^2)
names(state)
CARTArea = rpart(Life.Exp~Area, data=state, minbucket=1)
prp(CARTArea)
prp(CARTArea)
CARTAreaPreds = predict(CARTArea)
sum((CARTAreaPreds-state$Life.Exp^2)
)
sum((CARTAreaPreds-state$Life.Exp)^2)
nrow(state)
sum((CARTAreaPreds-state$Life.Exp)^2)/50
library(caret)
library(e1071)
set.seed(111)
set.seed(111)
tr.control=trainControl(method="cv", number=10)
cp.grid =expand.grid(.cp=seq(0.01,0.5,0.01))
train(Life.Exp~.,data=state,method="rpart",trControl=tr.control,tuneGrid=cp.grid)
tunedCART = rpart(Life.Exp~., datat=state, cp=0.12)
tunedCART = rpart(Life.Exp~., data=state, cp=0.12)
prp(tunedCART)
file.edit('HW4.Rmd')
states= read.csv('statedataSimple.csv')
head(states)
fullLinMod = lm(Life.Exp~, data=states)
fullLinMod = lm(Life.Exp~., data=states)
summary(fullLinMod)
fullLinMod = lm(Life.Exp~., data=states)
summary(fullLinMod)
sum(fullLinModPreds$residuals)^2)
fullLinMod = lm(Life.Exp~., data=states)
summary(fullLinMod)
sum((fullLinModPreds$residuals)^2)
fullLinMod = lm(Life.Exp~., data=states)
summary(fullLinMod)
sum((fullLinMod$residuals)^2)
signifAndAlmostSignifLinMod = lm(Life.Exp~Population, Murder, Frost, HS.Grad, data=states)
signifAndAlmostSignifLinMod = lm(Life.Exp~Population+ Murder+Frost+HS.Grad, data=states)
summary(signifAndAlmostSignifLinMod)
fullLinMod = lm(Life.Exp~., data=states)
summary(fullLinMod)
sum((fullLinMod$residuals)^2)
signifAndAlmostSignifLinMod = lm(Life.Exp~Population+ Murder+Frost+HS.Grad, data=states)
summary(signifAndAlmostSignifLinMod)
sum((signifAndAlmostSignifLinMod$residuals)^2)
cor(states)
cor(states$Life.Exp,states$Area)
?rpart
round(20/3)
FullCART = rpart(Life.Exp~., data=states)
prp(FullCART)
cor(states)
cor(states)[4,]
cor(states)[4,]%>%str
cor(states)[4,]
cor(states)[4,]
FullCARTPreds = predict(FullCART)
sum((FullCARTPreds - states$Life.Exp)^2)
fullLinMod = lm(Life.Exp~., data=states)
summary(fullLinMod)
sum((fullLinMod$residuals)^2)
signifAndAlmostSignifLinMod = lm(Life.Exp~Population+ Murder+Frost+HS.Grad, data=states)
summary(signifAndAlmostSignifLinMod)
sum((signifAndAlmostSignifLinMod$residuals)^2)
signifAndAlmostSignifLinMod = lm(Life.Exp~Population+ Murder+Frost+HS.Grad, data=states)
summary(signifAndAlmostSignifLinMod)
sum((signifAndAlmostSignifLinMod$residuals)^2)
FullCARTSmallestMinbucket = rpart(Life.Exp~., data=states, minbucket=1)
prp(FullCARTSmallestMinbucket)
FullCARTSmallestMinbucketPreds = predict(FullCARTSmallestMinbucket)
sum((FullCARTSmallestMinbucketPreds - states$Life.Exp)^2)
FullCARTSmallestMinbucket = rpart(Life.Exp~., data=states, minbucket=1)
prp(FullCARTSmallestMinbucket)
FullCARTSmallestMinbucketPreds = predict(FullCARTSmallestMinbucket)
sum((FullCARTSmallestMinbucketPreds - states$Life.Exp)^2)
FullCARTSmallestMinbucket = rpart(Life.Exp~., data=states, minbucket=1)
prp(FullCARTSmallestMinbucket, digits=4)
FullCARTSmallestMinbucketPreds = predict(FullCARTSmallestMinbucket)
sum((FullCARTSmallestMinbucketPreds - states$Life.Exp)^2)
prp(FullCARTSmallestMinbucket, digits=4)
statesForest = randomForest(Life.Exp~., data=states)
rfPreds = predict(statesForest, states$Life.Exp)
statesForest = randomForest(Life.Exp~., data=states)
rfPreds = predict(statesForest)
sum((rfPreds-states$Life.Exp)^2)
vu = varUsed(statesForest, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(statesForest$forest$xlevels[vusorted$ix]))
dotchart(vusorted$x, names(statesForest$forest$xlevels[vusorted$ix]))
varImpPlot(statesForest)
murderOnlyminBucketSmallest = rpart(Life.Exp~Murder, data=states, minbucket=1)
murderOnlyminBucketSmallestPreds = predict(murderOnlyminBucketSmallest)
sum((states$Life.Exp - murderOnlyminBucketSmallestPreds)^2)
onlyAreaCART = rpart(Life.Exp~Area, data=states)
prp(onlyAreaCART)
onlyAreaCARTPreds = predict(onlyAreaCART)
sum((states$Life.Exp - onlyAreaCARTPreds)^2)
prp(onlyAreaCART)
onlyAreaCARTSmallestMinbucket = rpart(Life.Exp~Area, data=states, minbucket=1)
prp(onlyAreaCARTSmallestMinbucket)
onlyAreaCARTSmallestMinbucketPreds = predict(onlyAreaCARTSmallestMinbucket)
sum((states$Life.Exp - onlyAreaCARTSmallestMinbucketPreds)^2)
prp(onlyAreaCARTSmallestMinbucket)
set.seed(111)
tr.control=trainControl(method="cv", number=10)
cp.grid =expand.grid(.cp=seq(0.01,0.5,0.01))
train(Life.Exp~.,data=states,method="rpart",trControl=tr.control,tuneGrid=cp.grid)
tunedCARTmodel = rpart(Life.Exp~., data=states, cp=0.12)
prp(tunedCARTmodel)
prp(tunedCARTmodel)
sum((tunedCARTmodelPreds-states$Life.Exp)^2)
tunedCARTmodelPreds = predict(tunedCARTmodel)
sum((tunedCARTmodelPreds-states$Life.Exp)^2)
set.seed(111)
tr.control=trainControl(method="cv", number=10)
cp.grid =expand.grid(.cp=seq(0.01,0.5,0.01))
train(Life.Exp~Area,data=states,method="rpart",trControl=tr.control,tuneGrid=cp.grid)
tunedCARTAreaOnly = rpart(Life.Exp~Area, data=states, cp=0.01)
prp(tunedCARTAreaOnly)
tunedCARTAreaOnlyPreds = predict(tunedCARTAreaOnly)
sum((tunedCARTAreaOnlyPreds-states$Life.Exp)^2)
prp(tunedCARTAreaOnly)
rm(list=ls())
